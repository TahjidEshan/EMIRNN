{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using EMI-GRU on the HAR Dataset\n",
    "\n",
    "This is a very simple example of how the existing EMI-RNN implementation can be used on the HAR dataset. We illustrate how to train a model that predicts on 48 step sequence in place of the 128 length baselines while attempting to predict early. For more advanced use cases which involves more sophisticated computation graphs or loss functions, please refer to the doc strings provided with the released code.\n",
    "\n",
    "In the preprint of our work, we use the terms *bag* and *instance* to refer to the LSTM input sequence of original length and the shorter ones we want to learn to predict on, respectively. In the code though, *bag* is replaced with *instance* and *instance* is replaced with *sub-instance*. We will use the term *instance* and *sub-instance* interchangeably.\n",
    "\n",
    "The network used here is a simple LSTM + Linear classifier network. \n",
    "\n",
    "The UCI [Human Activity Recognition](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T14:17:51.796585Z",
     "start_time": "2018-12-14T14:17:49.648375Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# Making sure edgeml is part of python path\n",
    "sys.path.insert(0, '../../')\n",
    "#For processing on CPU.\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='0'\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "# MI-RNN and EMI-RNN imports\n",
    "from edgeml.graph.rnn import EMI_DataPipeline\n",
    "from edgeml.graph.rnn import EMI_GRU\n",
    "from edgeml.trainer.emirnnTrainer import EMI_Trainer, EMI_Driver\n",
    "import edgeml.utils\n",
    "\n",
    "import keras.backend as K\n",
    "cfg = K.tf.ConfigProto()\n",
    "cfg.gpu_options.allow_growth = True\n",
    "K.set_session(K.tf.Session(config=cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us set up some network parameters for the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T14:17:51.803381Z",
     "start_time": "2018-12-14T14:17:51.798799Z"
    }
   },
   "outputs": [],
   "source": [
    "# Network parameters for our LSTM + FC Layer\n",
    "NUM_HIDDEN = 32\n",
    "NUM_TIMESTEPS = 48\n",
    "ORIGINAL_NUM_TIMESTEPS = 128\n",
    "NUM_FEATS = 9\n",
    "FORGET_BIAS = 1.0\n",
    "NUM_OUTPUT = 6\n",
    "USE_DROPOUT = True\n",
    "KEEP_PROB = 0.75\n",
    "\n",
    "# For dataset API\n",
    "PREFETCH_NUM = 5\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Number of epochs in *one iteration*\n",
    "NUM_EPOCHS = 2\n",
    "# Number of iterations in *one round*. After each iteration,\n",
    "# the model is dumped to disk. At the end of the current\n",
    "# round, the best model among all the dumped models in the\n",
    "# current round is picked up..\n",
    "NUM_ITER = 4\n",
    "# A round consists of multiple training iterations and a belief\n",
    "# update step using the best model from all of these iterations\n",
    "NUM_ROUNDS = 10\n",
    "LEARNING_RATE=0.001\n",
    "\n",
    "# A staging direcory to store models\n",
    "MODEL_PREFIX = 'HAR/model-gru'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Loading Data\n",
    "\n",
    "Please make sure the data is preprocessed to a format that is compatible with EMI-RNN. `tf/examples/EMI-RNN/fetch_har.py` can be used to download and setup the HAR dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T14:17:52.040352Z",
     "start_time": "2018-12-14T14:17:51.805319Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape is: (6355, 6, 48, 9)\n",
      "y_train shape is: (6355, 6, 6)\n",
      "x_test shape is: (997, 6, 48, 9)\n",
      "y_test shape is: (997, 6, 6)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "x_train, y_train = np.load('./HAR/48_16/x_train.npy'), np.load('./HAR/48_16/y_train.npy')\n",
    "x_test, y_test = np.load('./HAR/48_16/x_test.npy'), np.load('./HAR/48_16/y_test.npy')\n",
    "x_val, y_val = np.load('./HAR/48_16/x_val.npy'), np.load('./HAR/48_16/y_val.npy')\n",
    "\n",
    "# BAG_TEST, BAG_TRAIN, BAG_VAL represent bag_level labels. These are used for the label update\n",
    "# step of EMI/MI RNN\n",
    "BAG_TEST = np.argmax(y_test[:, 0, :], axis=1)\n",
    "BAG_TRAIN = np.argmax(y_train[:, 0, :], axis=1)\n",
    "BAG_VAL = np.argmax(y_val[:, 0, :], axis=1)\n",
    "NUM_SUBINSTANCE = x_train.shape[1]\n",
    "print(\"x_train shape is:\", x_train.shape)\n",
    "print(\"y_train shape is:\", y_train.shape)\n",
    "print(\"x_test shape is:\", x_val.shape)\n",
    "print(\"y_test shape is:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation Graph\n",
    "\n",
    "![Parst Computation graph illustration](img/3PartsGraph.png)\n",
    "\n",
    "The *EMI-RNN* computation graph is constructed out of the following three mutually disjoint parts:\n",
    "\n",
    "1. `EMI_DataPipeline`: An efficient data input pipeline that using the Tensorflow Dataset API. This module ingests data compatible with EMI-RNN and provides two iterators for a batch of input data, $x$ and label $y$. \n",
    "2. `EMI_RNN`: The 'abstract' `EMI-RNN` class defines the methods and attributes required for the forward computation graph. An implementation based on LSTM - `EMI_LSTM` is used in this document, though the user is free to implement his own computation graphs compatible with `EMI-RNN`. This module expects two Dataset API iterators for $x$-batch and $y$-batch as inputs and constructs the forward computation graph based on them. Every implementation of this class defines an `output` operation - the output of the forward computation graph.\n",
    "3. `EMI_Trainer`: An instance of `EMI_Trainer` class which defines the loss functions and the training routine. This expects an `output` operator from an `EMI-RNN` implementation and attaches loss functions and training routines to it.\n",
    "\n",
    "To build the computation graph, we create an instance of all the above and then connect them together.\n",
    "\n",
    "Note that, the `EMI_BasicLSTM` class is an implementation that uses an LSTM cell and pushes the LSTM output at each step to a secondary classifier for classification. This secondary classifier is not implemented as part of `EMI_BasicLSTM` and is left to the user to define by overriding the `createExtendedGraph` method, and the `restoreExtendedgraph` method.\n",
    "\n",
    "For the purpose of this example, we will be using a simple linear layer as a secondary classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T14:17:52.053161Z",
     "start_time": "2018-12-14T14:17:52.042928Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the linear secondary classifier\n",
    "def createExtendedGraph(self, baseOutput, *args, **kwargs):\n",
    "    W1 = tf.Variable(np.random.normal(size=[NUM_HIDDEN, NUM_OUTPUT]).astype('float32'), name='W1')\n",
    "    B1 = tf.Variable(np.random.normal(size=[NUM_OUTPUT]).astype('float32'), name='B1')\n",
    "    y_cap = tf.add(tf.tensordot(baseOutput, W1, axes=1), B1, name='y_cap_tata')\n",
    "    self.output = y_cap\n",
    "    self.graphCreated = True\n",
    "\n",
    "def restoreExtendedGraph(self, graph, *args, **kwargs):\n",
    "    y_cap = graph.get_tensor_by_name('y_cap_tata:0')\n",
    "    self.output = y_cap\n",
    "    self.graphCreated = True\n",
    "    \n",
    "def feedDictFunc(self, keep_prob=None, inference=False, **kwargs):\n",
    "    if inference is False:\n",
    "        feedDict = {self._emiGraph.keep_prob: keep_prob}\n",
    "    else:\n",
    "        feedDict = {self._emiGraph.keep_prob: 1.0}\n",
    "    return feedDict\n",
    "    \n",
    "EMI_GRU._createExtendedGraph = createExtendedGraph\n",
    "EMI_GRU._restoreExtendedGraph = restoreExtendedGraph\n",
    "\n",
    "if USE_DROPOUT is True:\n",
    "    EMI_Driver.feedDictFunc = feedDictFunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T14:17:52.335299Z",
     "start_time": "2018-12-14T14:17:52.055483Z"
    }
   },
   "outputs": [],
   "source": [
    "inputPipeline = EMI_DataPipeline(NUM_SUBINSTANCE, NUM_TIMESTEPS, NUM_FEATS, NUM_OUTPUT)\n",
    "emiGRU = EMI_GRU(NUM_SUBINSTANCE, NUM_HIDDEN, NUM_TIMESTEPS, NUM_FEATS,\n",
    "                        useDropout=USE_DROPOUT)\n",
    "emiTrainer = EMI_Trainer(NUM_TIMESTEPS, NUM_OUTPUT, lossType='xentropy',\n",
    "                         stepSize=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the elementary parts of the computation graph setup, we connect them together to form the forward graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T14:18:05.031382Z",
     "start_time": "2018-12-14T14:17:52.338750Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "g1 = tf.Graph()    \n",
    "with g1.as_default():\n",
    "    # Obtain the iterators to each batch of the data\n",
    "    x_batch, y_batch = inputPipeline()\n",
    "    # Create the forward computation graph based on the iterators\n",
    "    y_cap = emiGRU(x_batch)\n",
    "    # Create loss graphs and training routines\n",
    "    emiTrainer(y_cap, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMI Driver\n",
    "\n",
    "The `EMI_Driver` implements the `EMI_RNN` algorithm. For more information on how the driver works, please refer to `tf/docs/EMI-RNN.md`.\n",
    "\n",
    "Note that, during the training period, the accuracy printed is instance level accuracy with the current label information as target. Bag level accuracy, with which we are actually concerned, is calculated after the training ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T14:35:15.209910Z",
     "start_time": "2018-12-14T14:18:05.034359Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update policy: top-k\n",
      "Training with MI-RNN loss for 5 rounds\n",
      "Round: 0\n",
      "Epoch   1 Batch   191 (  390) Loss 0.00462 Acc 0.86979 | Val acc 0.98356 | Model saved to HAR/model-gru, global_step 1000\n",
      "Epoch   1 Batch   191 (  390) Loss 0.00372 Acc 0.89583 | Val acc 0.98324 | Model saved to HAR/model-gru, global_step 1001\n",
      "Epoch   1 Batch   191 (  390) Loss 0.00456 Acc 0.87500 | Val acc 0.98503 | Model saved to HAR/model-gru, global_step 1002\n",
      "Epoch   1 Batch   191 (  390) Loss 0.00349 Acc 0.88542 | Val acc 0.97103 | Model saved to HAR/model-gru, global_step 1003\n",
      "INFO:tensorflow:Restoring parameters from HAR/model-gru-1002\n",
      "Round: 1\n",
      "Epoch   1 Batch   191 (  390) Loss 0.00387 Acc 0.89062 | Val acc 0.98568 | Model saved to HAR/model-gru, global_step 1004\n",
      "Epoch   1 Batch   191 (  390) Loss 0.00388 Acc 0.89583 | Val acc 0.98503 | Model saved to HAR/model-gru, global_step 1005\n",
      "Epoch   1 Batch   191 (  390) Loss 0.00386 Acc 0.90625 | Val acc 0.98665 | Model saved to HAR/model-gru, global_step 1006\n",
      "Epoch   1 Batch   191 (  390) Loss 0.00352 Acc 0.89062 | Val acc 0.99691 | Model saved to HAR/model-gru, global_step 1007\n",
      "INFO:tensorflow:Restoring parameters from HAR/model-gru-1007\n",
      "Round: 2\n",
      "Epoch   1 Batch   191 (  390) Loss 0.00299 Acc 0.90625 | Val acc 0.98470 | Model saved to HAR/model-gru, global_step 1008\n",
      "Epoch   1 Batch   191 (  390) Loss 0.00319 Acc 0.89583 | Val acc 0.98438 | Model saved to HAR/model-gru, global_step 1009\n",
      "Epoch   1 Batch   191 (  390) Loss 0.00307 Acc 0.90625 | Val acc 0.98486 | Model saved to HAR/model-gru, global_step 1010\n",
      "Epoch   1 Batch   191 (  390) Loss 0.00289 Acc 0.91146 | Val acc 0.99723 | Model saved to HAR/model-gru, global_step 1011\n",
      "INFO:tensorflow:Restoring parameters from HAR/model-gru-1011\n",
      "Round: 3\n",
      "Epoch   1 Batch   191 (  390) Loss 0.00251 Acc 0.94271 | Val acc 0.98486 | Model saved to HAR/model-gru, global_step 1012\n",
      "Epoch   1 Batch   191 (  390) Loss 0.00194 Acc 0.96354 | Val acc 0.99870 | Model saved to HAR/model-gru, global_step 1013\n",
      "Epoch   1 Batch   191 (  390) Loss 0.00187 Acc 0.95833 | Val acc 0.98438 | Model saved to HAR/model-gru, global_step 1014\n",
      "Epoch   1 Batch   191 (  390) Loss 0.00162 Acc 0.97396 | Val acc 0.98454 | Model saved to HAR/model-gru, global_step 1015\n",
      "INFO:tensorflow:Restoring parameters from HAR/model-gru-1013\n",
      "Round: 4\n",
      "Epoch   1 Batch   191 (  390) Loss 0.00212 Acc 0.95312 | Val acc 0.99886 | Model saved to HAR/model-gru, global_step 1016\n",
      "Epoch   1 Batch   191 (  390) Loss 0.00170 Acc 0.96875 | Val acc 0.99723 | Model saved to HAR/model-gru, global_step 1017\n",
      "Epoch   1 Batch   191 (  390) Loss 0.00117 Acc 0.98438 | Val acc 0.99593 | Model saved to HAR/model-gru, global_step 1018\n",
      "Epoch   1 Batch   191 (  390) Loss 0.00097 Acc 0.98958 | Val acc 0.98405 | Model saved to HAR/model-gru, global_step 1019\n",
      "INFO:tensorflow:Restoring parameters from HAR/model-gru-1016\n",
      "Round: 5\n",
      "Switching to EMI-Loss function\n",
      "Epoch   1 Batch   191 (  390) Loss 0.24113 Acc 0.91146 | Val acc 0.99902 | Model saved to HAR/model-gru, global_step 1020\n",
      "Epoch   1 Batch   191 (  390) Loss 0.22029 Acc 0.97396 | Val acc 0.99886 | Model saved to HAR/model-gru, global_step 1021\n",
      "Epoch   1 Batch   191 (  390) Loss 0.19046 Acc 0.97917 | Val acc 0.99919 | Model saved to HAR/model-gru, global_step 1022\n",
      "Epoch   1 Batch   191 (  390) Loss 0.15364 Acc 0.98438 | Val acc 0.99886 | Model saved to HAR/model-gru, global_step 1023\n",
      "INFO:tensorflow:Restoring parameters from HAR/model-gru-1022\n",
      "Round: 6\n",
      "Epoch   1 Batch   191 (  390) Loss 0.16908 Acc 0.97917 | Val acc 0.99886 | Model saved to HAR/model-gru, global_step 1024\n",
      "Epoch   1 Batch   191 (  390) Loss 0.16444 Acc 0.98438 | Val acc 0.99609 | Model saved to HAR/model-gru, global_step 1025\n",
      "Epoch   1 Batch   191 (  390) Loss 0.13625 Acc 0.97396 | Val acc 0.99235 | Model saved to HAR/model-gru, global_step 1026\n",
      "Epoch   1 Batch   191 (  390) Loss 0.14270 Acc 0.95312 | Val acc 0.98942 | Model saved to HAR/model-gru, global_step 1027\n",
      "INFO:tensorflow:Restoring parameters from HAR/model-gru-1024\n",
      "Round: 7\n",
      "Epoch   1 Batch   191 (  390) Loss 0.16779 Acc 0.99479 | Val acc 0.99528 | Model saved to HAR/model-gru, global_step 1028\n",
      "Epoch   1 Batch   191 (  390) Loss 0.15521 Acc 0.99479 | Val acc 0.99186 | Model saved to HAR/model-gru, global_step 1029\n",
      "Epoch   1 Batch   191 (  390) Loss 0.12590 Acc 0.97917 | Val acc 0.99072 | Model saved to HAR/model-gru, global_step 1030\n",
      "Epoch   1 Batch   191 (  390) Loss 0.14737 Acc 0.94792 | Val acc 0.98991 | Model saved to HAR/model-gru, global_step 1031\n",
      "INFO:tensorflow:Restoring parameters from HAR/model-gru-1028\n",
      "Round: 8\n",
      "Epoch   1 Batch   191 (  390) Loss 0.14311 Acc 0.99479 | Val acc 0.99023 | Model saved to HAR/model-gru, global_step 1032\n",
      "Epoch   1 Batch   191 (  390) Loss 0.14519 Acc 0.97396 | Val acc 0.99007 | Model saved to HAR/model-gru, global_step 1033\n",
      "Epoch   1 Batch   191 (  390) Loss 0.13965 Acc 0.97396 | Val acc 0.98877 | Model saved to HAR/model-gru, global_step 1034\n",
      "Epoch   1 Batch   191 (  390) Loss 0.12842 Acc 0.96875 | Val acc 0.98568 | Model saved to HAR/model-gru, global_step 1035\n",
      "INFO:tensorflow:Restoring parameters from HAR/model-gru-1032\n",
      "Round: 9\n",
      "Epoch   1 Batch   191 (  390) Loss 0.13475 Acc 0.98958 | Val acc 0.99040 | Model saved to HAR/model-gru, global_step 1036\n",
      "Epoch   1 Batch   191 (  390) Loss 0.13162 Acc 0.95833 | Val acc 0.98275 | Model saved to HAR/model-gru, global_step 1037\n",
      "Epoch   1 Batch   191 (  390) Loss 0.13156 Acc 0.96354 | Val acc 0.98486 | Model saved to HAR/model-gru, global_step 1038\n",
      "Epoch   1 Batch   191 (  390) Loss 0.12347 Acc 0.97396 | Val acc 0.98617 | Model saved to HAR/model-gru, global_step 1039\n",
      "INFO:tensorflow:Restoring parameters from HAR/model-gru-1036\n"
     ]
    }
   ],
   "source": [
    "with g1.as_default():\n",
    "    emiDriver = EMI_Driver(inputPipeline, emiGRU, emiTrainer)\n",
    "\n",
    "emiDriver.initializeSession(g1)\n",
    "y_updated, modelStats = emiDriver.run(numClasses=NUM_OUTPUT, x_train=x_train,\n",
    "                                      y_train=y_train, bag_train=BAG_TRAIN,\n",
    "                                      x_val=x_val, y_val=y_val, bag_val=BAG_VAL,\n",
    "                                      numIter=NUM_ITER, keep_prob=KEEP_PROB,\n",
    "                                      numRounds=NUM_ROUNDS, batchSize=BATCH_SIZE,\n",
    "                                      numEpochs=NUM_EPOCHS, modelPrefix=MODEL_PREFIX,\n",
    "                                      fracEMI=0.5, updatePolicy='top-k', k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the  trained model\n",
    "\n",
    "![MIML Formulation illustration](img/MIML_illustration.png)\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "Since the trained model predicts on a smaller 48-step input while our test data has labels for 128 step inputs (i.e. bag level labels), evaluating the accuracy of the trained model is not straight forward. We perform the evaluation as follows:\n",
    "\n",
    "1. Divide the test data also into sub-instances; similar to what was done for the train data.\n",
    "2. Obtain sub-instance level predictions for each bag in the test data.\n",
    "3. Obtain bag level predictions from sub-instance level predictions. For this, we use our estimate of the length of the signature to estimate the expected number of sub-instances that would be non negative - $k$ illustrated in the figure. If a bag has $k$ consecutive sub-instances with the same label, that becomes the label of the bag. All other bags are labeled negative.\n",
    "4. Compare the predicted bag level labels with the known bag level labels in test data.\n",
    "\n",
    "## Early Savings\n",
    "\n",
    "Early prediction is accomplished by defining an early prediction policy method. This method receives the prediction at each step of the learned LSTM for a sub-instance as input and is expected to return a predicted class and the 0-indexed step at which it made this prediction. This is illustrated below in code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T14:35:15.218040Z",
     "start_time": "2018-12-14T14:35:15.211771Z"
    }
   },
   "outputs": [],
   "source": [
    "# Early Prediction Policy: We make an early prediction based on the predicted classes\n",
    "#     probability. If the predicted class probability > minProb at some step, we make\n",
    "#     a prediction at that step.\n",
    "def earlyPolicy_minProb(instanceOut, minProb, **kwargs):\n",
    "    assert instanceOut.ndim == 2\n",
    "    classes = np.argmax(instanceOut, axis=1)\n",
    "    prob = np.max(instanceOut, axis=1)\n",
    "    index = np.where(prob >= minProb)[0]\n",
    "    if len(index) == 0:\n",
    "        assert (len(instanceOut) - 1) == (len(classes) - 1)\n",
    "        return classes[-1], len(instanceOut) - 1\n",
    "    index = index[0]\n",
    "    return classes[index], index\n",
    "\n",
    "def getEarlySaving(predictionStep, numTimeSteps, returnTotal=False):\n",
    "    predictionStep = predictionStep + 1\n",
    "    predictionStep = np.reshape(predictionStep, -1)\n",
    "    totalSteps = np.sum(predictionStep)\n",
    "    maxSteps = len(predictionStep) * numTimeSteps\n",
    "    savings = 1.0 - (totalSteps / maxSteps)\n",
    "    if returnTotal:\n",
    "        return savings, totalSteps\n",
    "    return savings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T14:35:16.257489Z",
     "start_time": "2018-12-14T14:35:15.221029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at k = 2: 0.932474\n",
      "Savings due to MI-RNN : 0.625000\n",
      "Savings due to Early prediction: 0.714685\n",
      "Total Savings: 0.893007\n"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "predictions, predictionStep = emiDriver.getInstancePredictions(x_test, y_test, earlyPolicy_minProb,\n",
    "                                                               minProb=0.99, keep_prob=1.0)\n",
    "bagPredictions = emiDriver.getBagPredictions(predictions, minSubsequenceLen=k, numClass=NUM_OUTPUT)\n",
    "print('Accuracy at k = %d: %f' % (k,  np.mean((bagPredictions == BAG_TEST).astype(int))))\n",
    "mi_savings = (1 - NUM_TIMESTEPS / ORIGINAL_NUM_TIMESTEPS)\n",
    "emi_savings = getEarlySaving(predictionStep, NUM_TIMESTEPS)\n",
    "total_savings = mi_savings + (1 - mi_savings) * emi_savings\n",
    "print('Savings due to MI-RNN : %f' % mi_savings)\n",
    "print('Savings due to Early prediction: %f' % emi_savings)\n",
    "print('Total Savings: %f' % (total_savings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T14:35:17.044115Z",
     "start_time": "2018-12-14T14:35:16.259280Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   len       acc  macro-fsc  macro-pre  macro-rec  micro-fsc  micro-pre  \\\n",
      "0    1  0.919919   0.920252   0.921175   0.922145   0.919919   0.919919   \n",
      "1    2  0.932474   0.933665   0.933282   0.934488   0.932474   0.932474   \n",
      "2    3  0.928402   0.929679   0.930218   0.930279   0.928402   0.928402   \n",
      "3    4  0.902613   0.904272   0.913013   0.903145   0.902613   0.902613   \n",
      "4    5  0.884628   0.887599   0.905353   0.884241   0.884628   0.884628   \n",
      "5    6  0.868680   0.873429   0.900421   0.867785   0.868680   0.868680   \n",
      "\n",
      "   micro-rec  \n",
      "0   0.919919  \n",
      "1   0.932474  \n",
      "2   0.928402  \n",
      "3   0.902613  \n",
      "4   0.884628  \n",
      "5   0.868680  \n",
      "Max accuracy 0.932474 at subsequencelength 2\n",
      "Max micro-f 0.932474 at subsequencelength 2\n",
      "Micro-precision 0.932474 at subsequencelength 2\n",
      "Micro-recall 0.932474 at subsequencelength 2\n",
      "Max macro-f 0.933665 at subsequencelength 2\n",
      "macro-precision 0.933282 at subsequencelength 2\n",
      "macro-recall 0.934488 at subsequencelength 2\n"
     ]
    }
   ],
   "source": [
    "# A slightly more detailed analysis method is provided. \n",
    "df = emiDriver.analyseModel(predictions, BAG_TEST, NUM_SUBINSTANCE, NUM_OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking the best model\n",
    "\n",
    "The `EMI_Driver.run()` method, upon finishing, returns a list containing information about the best models after each EMI-RNN round. This can be used to identify the best model (based on validation accuracy) at the end of each round - illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-14T14:35:54.899340Z",
     "start_time": "2018-12-14T14:35:17.047464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from HAR/model-gru-1002\n",
      "Round:  0, Validation accuracy: 0.9850, Test Accuracy (k = 2): 0.912114, Total Savings: 0.809473\n",
      "INFO:tensorflow:Restoring parameters from HAR/model-gru-1007\n",
      "Round:  1, Validation accuracy: 0.9969, Test Accuracy (k = 2): 0.896166, Total Savings: 0.817911\n",
      "INFO:tensorflow:Restoring parameters from HAR/model-gru-1011\n",
      "Round:  2, Validation accuracy: 0.9972, Test Accuracy (k = 2): 0.909060, Total Savings: 0.824232\n",
      "INFO:tensorflow:Restoring parameters from HAR/model-gru-1013\n",
      "Round:  3, Validation accuracy: 0.9987, Test Accuracy (k = 2): 0.911435, Total Savings: 0.837865\n",
      "INFO:tensorflow:Restoring parameters from HAR/model-gru-1016\n",
      "Round:  4, Validation accuracy: 0.9989, Test Accuracy (k = 2): 0.917883, Total Savings: 0.841204\n",
      "INFO:tensorflow:Restoring parameters from HAR/model-gru-1022\n",
      "Round:  5, Validation accuracy: 0.9992, Test Accuracy (k = 2): 0.927723, Total Savings: 0.870538\n",
      "INFO:tensorflow:Restoring parameters from HAR/model-gru-1024\n",
      "Round:  6, Validation accuracy: 0.9989, Test Accuracy (k = 2): 0.926026, Total Savings: 0.878533\n",
      "INFO:tensorflow:Restoring parameters from HAR/model-gru-1028\n",
      "Round:  7, Validation accuracy: 0.9953, Test Accuracy (k = 2): 0.930438, Total Savings: 0.885675\n",
      "INFO:tensorflow:Restoring parameters from HAR/model-gru-1032\n",
      "Round:  8, Validation accuracy: 0.9902, Test Accuracy (k = 2): 0.931116, Total Savings: 0.890533\n",
      "INFO:tensorflow:Restoring parameters from HAR/model-gru-1036\n",
      "Round:  9, Validation accuracy: 0.9904, Test Accuracy (k = 2): 0.932474, Total Savings: 0.893007\n"
     ]
    }
   ],
   "source": [
    "devnull = open(os.devnull, 'r')\n",
    "for val in modelStats:\n",
    "    round_, acc, modelPrefix, globalStep = val\n",
    "    emiDriver.loadSavedGraphToNewSession(modelPrefix, globalStep, redirFile=devnull)\n",
    "    predictions, predictionStep = emiDriver.getInstancePredictions(x_test, y_test, earlyPolicy_minProb,\n",
    "                                                               minProb=0.99, keep_prob=1.0)\n",
    "\n",
    "    bagPredictions = emiDriver.getBagPredictions(predictions, minSubsequenceLen=k, numClass=NUM_OUTPUT)\n",
    "    print(\"Round: %2d, Validation accuracy: %.4f\" % (round_, acc), end='')\n",
    "    print(', Test Accuracy (k = %d): %f, ' % (k,  np.mean((bagPredictions == BAG_TEST).astype(int))), end='')\n",
    "    mi_savings = (1 - NUM_TIMESTEPS / ORIGINAL_NUM_TIMESTEPS)\n",
    "    emi_savings = getEarlySaving(predictionStep, NUM_TIMESTEPS)\n",
    "    total_savings = mi_savings + (1 - mi_savings) * emi_savings\n",
    "    print(\"Total Savings: %f\" % total_savings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
